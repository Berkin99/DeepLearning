{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **2. Neural Networks**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2.1 Perceptron**\n",
    "\n",
    "* **Perceptron** is building block of neural networks. \n",
    "* Frank Rosenblatt 1962\n",
    "\n",
    "1. **Inputs**: The perceptron receives one or more input values, often denoted as $x_1, x_2, \\dots, x_n$.\n",
    "\n",
    "2. **Weights**: Each input has a corresponding weight ($w_1, w_2, \\dots, w_n$) which indicates the importance of that input.\n",
    "\n",
    "3. **Weighted Sum**: The perceptron calculates a weighted sum of the inputs:\n",
    "   \n",
    "   $$\n",
    "   z = w_1 x_1 + w_2 x_2 + \\dots + w_n x_n + b\n",
    "   $$\n",
    "\n",
    "   Here, $b$ is the **bias** term, which helps shift the decision boundary.\n",
    "\n",
    "4. **Activation Function**: The weighted sum $z$ is passed through an activation function to produce the output. In a basic perceptron, this is usually a **step function**, which outputs a binary value (e.g., 0 or 1):\n",
    "   \n",
    "$$\n",
    "\\mathrm{output} =\n",
    "\\begin{cases}\n",
    "1 & \\text{if } z \\geq 0 \\\\\n",
    "0 & \\text{if } z < 0\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "#### **Functionality**\n",
    "\n",
    "**Regression** and **Classification**\n",
    "\n",
    "* The perceptron can be seen as a linear classifier that separates data into two classes using a **decision boundary** (a line, plane, or hyperplane depending on the number of dimensions). During the training process, the perceptron adjusts its weights and bias to correctly classify input data.\n",
    "\n",
    "#### **Limitations**\n",
    "- The basic perceptron can only solve **linearly separable problems** (e.g., problems where a single line can separate classes).\n",
    "- It cannot handle more complex tasks that require non-linear decision boundaries (like the XOR problem).\n",
    "\n",
    "#### **Usage and Evolution**\n",
    "- A single-layer perceptron is limited to simple classification tasks.\n",
    "- A **Multi-Layer Perceptron (MLP)**, which includes multiple layers of perceptrons with non-linear activation functions, can solve more complex tasks and is the foundation of modern neural networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2.2 Multiclass Perceptron with Softmax**\n",
    "\n",
    "### 1. **Model Structure**\n",
    "- Consider an input vector $ \\mathbf{x} = [x_1, x_2, \\dots, x_d] $, where $ d $ is the number of input features.\n",
    "- The model aims to classify $ \\mathbf{x} $ into one of $ k $ classes: $ y_1, y_2, \\dots, y_k $.\n",
    "- Each class $ y_i $ has a corresponding weight vector $ \\mathbf{w}_i = [w_{i1}, w_{i2}, \\dots, w_{id}] $ and a bias term $ w_{i0} $.\n",
    "\n",
    "### 2. **Score Calculation**\n",
    "- The **score** $ o_i $ for class $ y_i $ is computed as:\n",
    "\n",
    "$$\n",
    "o_i = \\mathbf{w}_i^T \\cdot \\mathbf{x} + w_{i0} = \\sum_{j=1}^{d} w_{ij} x_j + w_{i0}\n",
    "$$\n",
    "\n",
    "  where:\n",
    "  - $ \\mathbf{w}_i^T $ is the transpose of the weight vector for class $ y_i $.\n",
    "  - $ w_{i0} $ is the bias term for class $ y_i $.\n",
    "\n",
    "### 3. **Softmax Activation for Class Probabilities**\n",
    "- To convert the raw scores $ o_i $ into **class probabilities**, we apply the **softmax** function:\n",
    "\n",
    "$$\n",
    "y_i = \\frac{\\exp(o_i)}{\\sum_{j=1}^{k} \\exp(o_j)}\n",
    "$$\n",
    "\n",
    "  - $ \\exp $ is the exponential function.\n",
    "  - The softmax function ensures that the output probabilities sum to 1, making it a proper probability distribution over the classes.\n",
    "\n",
    "### 4. **Prediction**\n",
    "- The final predicted class $ \\hat{y} $ is the one with the highest probability:\n",
    "\n",
    "$$\n",
    "\\hat{y} = \\arg\\max_{k} y_k\n",
    "$$\n",
    "\n",
    "### 5. **Loss Function**\n",
    "- The Multiclass Perceptron with softmax typically uses the **Cross-Entropy Loss** to measure prediction error:\n",
    "\n",
    "$$\n",
    "L = -\\sum_{i=1}^{k} r_i \\log(y_i)\n",
    "$$\n",
    "\n",
    "  - Here, $ r_i $ is the true label for the class (usually represented as a one-hot vector, indicating the desired output).\n",
    "\n",
    "### 6. **Gradient Descent Weight Update**\n",
    "- The weights and biases are updated using **Gradient Descent** based on the Cross-Entropy Loss.\n",
    "- For weight $ w_{ij} $ (weight for the $ j $-th feature in class $ i $), the update $ \\Delta w_{ij} $ is given by:\n",
    "\n",
    "$$\n",
    "\\Delta w_{ij} = - \\eta \\frac{\\partial L}{\\partial w_{ij}}\n",
    "$$\n",
    "\n",
    "  where:\n",
    "  - $ \\eta $ is the **learning rate**.\n",
    "\n",
    "- The gradient with respect to $ w_{ij} $ can be computed as:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial w_{ij}} = (y_i - r_i) x_j\n",
    "$$\n",
    "\n",
    "  Therefore, the weight update becomes:\n",
    "\n",
    "$$\n",
    "\\Delta w_{ij} =\\eta (r_i - y_i) x_j\n",
    "$$\n",
    "\n",
    "- For the bias $ w_{i0} $, the update $ \\Delta w_{i0} $ is:\n",
    "\n",
    "$$\n",
    "\\Delta w_{i0} = - \\eta (y_i - r_i)\n",
    "$$\n",
    "\n",
    "### **Summary of Key Formulas**\n",
    "\n",
    "1. **Score Computation**:\n",
    "   \n",
    "$$\n",
    "o_i = \\sum_{j=1}^{d} w_{ij} x_j + w_{i0}\n",
    "$$\n",
    "\n",
    "2. **Softmax Probability**:\n",
    "   \n",
    "$$\n",
    "y_i = \\frac{\\exp(o_i)}{\\sum_{j=1}^{k} \\exp(o_j)}\n",
    "$$\n",
    "\n",
    "3. **Cross-Entropy Loss**:\n",
    "   \n",
    "$$\n",
    "L = -\\sum_{i=1}^{k} r_i \\log(y_i)\n",
    "$$\n",
    "\n",
    "4. **Weight Update**:\n",
    "   \n",
    "$$\n",
    "\\Delta w_{ij} = - \\eta (y_i - r_i) x_j\n",
    "$$\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
