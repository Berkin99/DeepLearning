{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **2. Neural Networks**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2.1 Perceptron**\n",
    "\n",
    "* **Perceptron** is building block of neural networks. \n",
    "* Frank Rosenblatt 1962\n",
    "\n",
    "1. **Inputs**: The perceptron receives one or more input values, often denoted as $x_1, x_2, \\dots, x_n$.\n",
    "\n",
    "2. **Weights**: Each input has a corresponding weight ($w_1, w_2, \\dots, w_n$) which indicates the importance of that input.\n",
    "\n",
    "3. **Weighted Sum**: The perceptron calculates a weighted sum of the inputs:\n",
    "   \n",
    "   $$\n",
    "   z = w_1 x_1 + w_2 x_2 + \\dots + w_n x_n + b\n",
    "   $$\n",
    "\n",
    "   Here, $b$ is the **bias** term, which helps shift the decision boundary.\n",
    "\n",
    "4. **Activation Function**: The weighted sum $z$ is passed through an activation function to produce the output. In a basic perceptron, this is usually a **step function**, which outputs a binary value (e.g., 0 or 1):\n",
    "   \n",
    "$$\n",
    "\\mathrm{output} =\n",
    "\\begin{cases}\n",
    "1 & \\text{if } z \\geq 0 \\\\\n",
    "0 & \\text{if } z < 0\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "#### **Functionality**\n",
    "\n",
    "**Regression** and **Classification**\n",
    "\n",
    "* The perceptron can be seen as a linear classifier that separates data into two classes using a **decision boundary** (a line, plane, or hyperplane depending on the number of dimensions). During the training process, the perceptron adjusts its weights and bias to correctly classify input data.\n",
    "\n",
    "#### **Limitations**\n",
    "- The basic perceptron can only solve **linearly separable problems** (e.g., problems where a single line can separate classes).\n",
    "- It cannot handle more complex tasks that require non-linear decision boundaries (like the XOR problem).\n",
    "\n",
    "#### **Usage and Evolution**\n",
    "- A single-layer perceptron is limited to simple classification tasks.\n",
    "- A **Multi-Layer Perceptron (MLP)**, which includes multiple layers of perceptrons with non-linear activation functions, can solve more complex tasks and is the foundation of modern neural networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2.2 Multiclass Perceptron**\n",
    "\n",
    "#### 1. **Model Structure**\n",
    "- Consider an input vector $ \\mathbf{x} = [x_1, x_2, \\dots, x_d] $, where $ d $ is the number of input features.\n",
    "- The model aims to classify $ \\mathbf{x} $ into one of $ k $ classes: $ y_1, y_2, \\dots, y_k $.\n",
    "- Each class $ y_i $ has a corresponding weight vector $ \\mathbf{w}_i = [w_{i1}, w_{i2}, \\dots, w_{id}] $ and a bias term $ w_{i0} $.\n",
    "\n",
    "#### 2. **Score Calculation**\n",
    "- The **score** $ o_i $ for class $ y_i $ is computed as:\n",
    "\n",
    "$$\n",
    "o_i = \\mathbf{w}_i^T \\cdot \\mathbf{x} + w_{i0} = \\sum_{j=1}^{d} w_{ij} x_j + w_{i0}\n",
    "$$\n",
    "\n",
    "  where:\n",
    "  - $ \\mathbf{w}_i^T $ is the transpose of the weight vector for class $ y_i $.\n",
    "  - $ w_{i0} $ is the bias term for class $ y_i $.\n",
    "\n",
    "#### 3. **Softmax Activation for Class Probabilities**\n",
    "- To convert the raw scores $ o_i $ into **class probabilities**, we apply the **softmax** function:\n",
    "\n",
    "$$\n",
    "y_i = \\frac{\\exp(o_i)}{\\sum_{j=1}^{k} \\exp(o_j)}\n",
    "$$\n",
    "\n",
    "  - $ \\exp $ is the exponential function.\n",
    "  - The softmax function ensures that the output probabilities sum to 1, making it a proper probability distribution over the classes.\n",
    "\n",
    "#### 4. **Prediction**\n",
    "- The final predicted class $ \\hat{y} $ is the one with the highest probability:\n",
    "\n",
    "$$\n",
    "\\hat{y} = \\arg\\max_{i} y_i\n",
    "$$\n",
    "\n",
    "#### 5. **Loss Function**\n",
    "\n",
    "- The Multiclass Perceptron with softmax typically uses the **Cross-Entropy Loss** to measure prediction error:\n",
    "\n",
    "$$\n",
    "L = -\\sum_{i=1}^{k} r_i \\log(y_i)\n",
    "$$\n",
    "\n",
    "  - Here, $ r_i $ is the true label for the class (usually represented as a one-hot vector, indicating the desired output).\n",
    "\n",
    "#### 6. **Gradient Descent Weight Update**\n",
    "- The weights and biases are updated using **Gradient Descent** based on the Cross-Entropy Loss.\n",
    "- For weight $ w_{ij} $ (weight for the $ j $-th feature in class $ i $), the update $ \\Delta w_{ij} $ is given by:\n",
    "\n",
    "$$\n",
    "\\Delta w_{ij} = - \\eta \\frac{\\partial L}{\\partial w_{ij}}\n",
    "$$\n",
    "\n",
    "  where:\n",
    "  - $ \\eta $ is the **learning rate**.\n",
    "\n",
    "- The gradient with respect to $ w_{ij} $ can be computed as:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial w_{ij}} = (y_i - r_i) x_j\n",
    "$$\n",
    "\n",
    "  Therefore, the weight update becomes:\n",
    "\n",
    "$$\n",
    "\\Delta w_{ij} = \\eta (r_i - y_i) x_j\n",
    "$$\n",
    "\n",
    "- For the bias $ w_{i0} $, the update $ \\Delta w_{i0} $ is:\n",
    "\n",
    "$$\n",
    "\\Delta w_{i0} = \\eta (r_i - y_i)\n",
    "$$\n",
    "\n",
    "#### **Summary of Key Formulas**\n",
    "\n",
    "1. **Score Computation**:\n",
    "   \n",
    "$$\n",
    "o_i = \\sum_{j=1}^{d} w_{ij} x_j + w_{i0}\n",
    "$$\n",
    "\n",
    "2. **Softmax Probability**:\n",
    "   \n",
    "$$\n",
    "y_i = \\frac{\\exp(o_i)}{\\sum_{j=1}^{k} \\exp(o_j)}\n",
    "$$\n",
    "\n",
    "3. **Cross-Entropy Loss**:\n",
    "   \n",
    "$$\n",
    "L = -\\sum_{i=1}^{k} r_i \\log(y_i)\n",
    "$$\n",
    "\n",
    "4. **Weight Update**:\n",
    "   \n",
    "$$\n",
    "\\Delta w_{ij} = \\eta (r_i - y_i) x_j\n",
    "$$\n",
    "\n",
    "5. **Bias Update**:\n",
    "\n",
    "$$\n",
    "\\Delta w_{i0} = \\eta (r_i - y_i)\n",
    "$$\n",
    "\n",
    "#### **Additional Considerations**\n",
    "- This framework is applicable for multiclass classification problems where each input $ \\mathbf{x} $ needs to be classified into one of several categories.\n",
    "- The Cross-Entropy Loss penalizes the model when the predicted probabilities diverge from the true labels, effectively guiding the weights during training to improve accuracy.\n",
    "\n",
    "#### 7. **Comparison with Binary Classification**\n",
    "- For binary classification, a sigmoid activation is typically used, leading to the Binary Cross-Entropy Loss:\n",
    "\n",
    "$$\n",
    "E^t(\\mathbf{w} \\mid \\mathbf{x}^t, r^t) = -r^t \\log y^t - (1 - r^t) \\log (1 - y^t)\n",
    "$$\n",
    "\n",
    "  - In contrast, the softmax function generalizes the sigmoid to multiple classes, where each class receives a score that is normalized to a probability.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2.3 Multilayer Perceptron**\n",
    "\n",
    "The provided image appears to illustrate a **neural network model**, specifically focusing on the **forward pass** and **backpropagation** for training. Here's a breakdown of the key components shown in the image:\n",
    "\n",
    "#### **Forward Pass : Feedforward**\n",
    "\n",
    "**Output Calculation :** The output for class $ y_i $ is given by:\n",
    "\n",
    "$$\n",
    "y_i = \\mathbf{v}_i^T \\mathbf{z} = \\sum_{h=1}^{H} v_{ih} z_h + v_{i0}\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $ y_i $ is the output for class $ i $.\n",
    "- $ \\mathbf{v}_i $ is the weight vector for the output layer corresponding to class $ i $.\n",
    "- $ \\mathbf{z} = [z_1, z_2, \\dots, z_H] $ is the vector of activations from the hidden layer.\n",
    "- $ H $ is the number of hidden units.\n",
    "- $ v_{ih} $ is the weight connecting the hidden unit $ h $ to the output unit $ i $.\n",
    "- $ v_{i0} $ is the bias term for the output unit $ i $.\n",
    "\n",
    "#### **Hidden Layer Activation**\n",
    "The activation of a hidden unit $ z_h $ is computed using the **sigmoid** function:\n",
    "\n",
    "$$\n",
    "z_h = \\text{sigmoid}(\\mathbf{w}_h^T \\mathbf{x}) = \\frac{1}{1 + \\exp\\left( - \\left( \\sum_{j=1}^{d} w_{hj} x_j + w_{h0} \\right) \\right)}\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $ z_h $ is the activation for the hidden unit $ h $.\n",
    "- $ \\mathbf{w}_h $ is the weight vector for the hidden layer, connecting input $ \\mathbf{x} $ to the hidden unit $ h $.\n",
    "- $ x_j $ are the input features.\n",
    "- $ d $ is the number of input features.\n",
    "- $ w_{hj} $ is the weight from the input feature $ j $ to the hidden unit $ h $.\n",
    "- $ w_{h0} $ is the bias for the hidden unit $ h $.\n",
    "\n",
    "#### **Backpropagation (Gradient Calculation)**\n",
    "\n",
    "The equation inside the red box represents the **chain rule** for computing the gradient of the error function $ E $ with respect to a weight $ w_{hj} $ in the hidden layer:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial E}{\\partial w_{hj}} = \\frac{\\partial E}{\\partial y_i} \\cdot \\frac{\\partial y_i}{\\partial z_h} \\cdot \\frac{\\partial z_h}{\\partial w_{hj}}\n",
    "$$\n",
    "\n",
    "This is how the gradient is computed step-by-step:\n",
    "- $ \\frac{\\partial E}{\\partial y_i} $: Gradient of the error function $ E $ with respect to the output $ y_i $. This measures how the error changes with the output.\n",
    "- $ \\frac{\\partial y_i}{\\partial z_h} $: Gradient of the output $ y_i $ with respect to the hidden unit activation $ z_h $. This measures how the output changes with the hidden activation.\n",
    "- $ \\frac{\\partial z_h}{\\partial w_{hj}} $: Gradient of the hidden activation $ z_h $ with respect to the weight $ w_{hj} $. This measures how the hidden activation changes with the weight.\n",
    "\n",
    "#### **What Does This Mean?**\n",
    "- The image essentially shows how to calculate the **output** of a neural network using a **hidden layer** and how to adjust the weights using **backpropagation** to minimize the error.\n",
    "- The formula for backpropagation relies on the **chain rule** to propagate the error gradient from the output back through the network to adjust the weights accordingly.\n",
    "\n",
    "This is a standard method used in training neural networks to iteratively reduce the prediction error by updating the weights based on the computed gradients.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2.4 Vanishing and Exploding Gradients**\n",
    "\n",
    "The problem arises due to the **chain rule** used in backpropagation:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial E}{\\partial w} = \\frac{\\partial E}{\\partial y} \\cdot \\frac{\\partial y}{\\partial z} \\cdot \\frac{\\partial z}{\\partial w}.\n",
    "$$\n",
    "\n",
    "For a deep network, gradients are repeatedly multiplied as they propagate backward through the layers. This repeated multiplication can lead to:\n",
    "\n",
    "#### **Vanishing Gradients**\n",
    "\n",
    "- **How it happens:** If activation functions (e.g., sigmoid, tanh) have small derivatives ($ \\frac{\\partial z}{\\partial w} \\approx 0 $), the gradients shrink **exponentially** as they flow backward through layers.\n",
    "- **Effect:** Gradients in earlier layers approach zero, so weights in those layers are barely updated.\n",
    "- **Result:** The network struggles to learn and converges very slowly.\n",
    "\n",
    "\n",
    "#### **Exploding Gradients**\n",
    "- **How it happens:** If the weights $ w $ or activation values are large, the derivatives in the chain rule become much larger than 1. When these large values are multiplied across layers, the gradients grow **exponentially**.\n",
    "- **Effect:** Gradients become excessively large, causing unstable updates to weights or numerical overflow.\n",
    "- **Result:** Training diverges or the model fails to converge.\n",
    "\n",
    "\n",
    "#### **Causes**\n",
    "1. **Weight Initialization:** \n",
    "   - If weights are not initialized carefully (e.g., too large or not scaled by the number of inputs), this amplifies the gradient explosion/vanishing effect.\n",
    "2. **Saturating Activation Functions:** \n",
    "   - Sigmoid and tanh functions squash their outputs into small ranges, leading to small derivatives, which worsen vanishing gradients.\n",
    "3. **Network Depth:** \n",
    "   - The more layers in the network, the more multiplications occur, amplifying vanishing/exploding gradients.\n",
    "\n",
    "#### **Key Insights**\n",
    "- **Vanishing gradients** → slow updates in early layers, poor learning.\n",
    "- **Exploding gradients** → unstable updates, divergence.\n",
    "\n",
    "#### **How To Fix**\n",
    "\n",
    "1. Better Weight Initialization (e.g., Xavier, He)\n",
    "2. Activation Function Selection (e.g., ReLU, Leaky ReLU)\n",
    "3. Gradient Clipping\n",
    "4. Batch Normalization\n",
    "5. Residual Connections (Skip Connections)\n",
    "6. Smaller Learning Rates\n",
    "7. Use of LSTM/GRU in RNNs\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
