{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **CSE655 Midterm Questions**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. PARAMETER CALCULATION**\n",
    "* Calculate Tunable parameters given CNN\n",
    "* Calculate Tunable parameters given RNN\n",
    "* Calculate Tunable parameters given Autoencoder\n",
    "* Calculate Tunable parameters given U-Net\n",
    "* Calculate Tunable parameters given Res-Net\n",
    "* Calculate Tunable parameters given LSTM\n",
    "\n",
    "**2. CHL**\n",
    "* **Explain Contrastive Hebbian Learning steps by iteratively.**\n",
    "\n",
    "**3. GAN**\n",
    "* **Draw a GAN Diagram for Image Generation**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **CHATGPT Final Exam Question Estimation :**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1. Explain the difference between Conditional Autoencoders and Variational Autoencoders.**  \n",
    "- a. How does the latent space representation differ between these two architectures?  \n",
    "- b. Provide an example use case where each would be suitable.  \n",
    "- c. How could combining these two architectures (Conditional and Variational) be useful?  \n",
    "\n",
    "### **2. Describe how Simulated Annealing is used in neural network weight optimization.**  \n",
    "- a. What are the advantages of using simulated annealing compared to gradient descent?  \n",
    "- b. How does the cooling schedule affect convergence speed and solution quality?  \n",
    "- c. Give an example of a problem where simulated annealing would outperform standard optimizers.  \n",
    "\n",
    "### **3. Explain the training process of Restricted Boltzmann Machines (RBMs).**  \n",
    "- a. What is the role of the hidden and visible layers in RBMs?  \n",
    "- b. How does Contrastive Divergence approximate the training objective?  \n",
    "- c. Explain how RBMs can be stacked to form Deep Belief Networks (DBNs).  \n",
    "\n",
    "### **4. Compare and contrast the KL Divergence and Wasserstein Distance in generative models.**  \n",
    "- a. Why is KL divergence often used in VAEs but not in GANs?  \n",
    "- b. How does Wasserstein Distance improve GAN training stability?  \n",
    "- c. Which of these metrics would you use for evaluating mode diversity in generative models, and why?  \n",
    "\n",
    "### **5. Describe the importance of Feature Embeddings in Recommendation Systems.**  \n",
    "- a. How are embeddings generated for users and items in collaborative filtering?  \n",
    "- b. Explain how embeddings can be fine-tuned during the recommendation process.  \n",
    "- c. What are the challenges of using pre-trained embeddings for recommendation systems?  \n",
    "\n",
    "### **6. Explain the advantages of Transformers over Recurrent Neural Networks.**  \n",
    "- a. How does self-attention allow Transformers to process sequences more efficiently?  \n",
    "- b. Why do Transformers perform better on long sequences compared to RNNs?  \n",
    "- c. Discuss a real-world example where Transformers outperform traditional sequence models.  \n",
    "\n",
    "### **7. Discuss Regularization Techniques in Generative Neural Networks.**  \n",
    "- a. What is the role of gradient penalty in Wasserstein GANs?  \n",
    "- b. How can spectral normalization improve the discriminator's stability?  \n",
    "- c. Explain why dropout is rarely used in GAN training compared to other regularization methods.  \n",
    "\n",
    "### **8. Calculate the parameters of a Transformer Encoder Block:**  \n",
    "- Input: Sequence length = 128, Embedding dimension = 512  \n",
    "- Multi-Head Attention: 8 heads, Key/Query/Value dimensions = 64  \n",
    "- Feedforward Layers: First layer = 2048 units, Second layer = 512 units  \n",
    "\n",
    "_Hint: Consider parameters for attention layers, linear transformations, and feedforward layers._  \n",
    "\n",
    "### **9. Explain the role of attention mechanisms in Variational Autoencoders.**  \n",
    "- a. How can attention be integrated into the encoder-decoder structure of VAEs?  \n",
    "- b. What are the benefits of using attention for latent variable modeling?  \n",
    "- c. Provide a use case where attention-augmented VAEs would outperform standard VAEs.  \n",
    "\n",
    "### **10. How does Contrastive Hebbian Learning differ from Backpropagation?**  \n",
    "- a. What is the biological motivation behind Contrastive Hebbian Learning?  \n",
    "- b. How does the update rule in CHL approximate gradient descent?  \n",
    "- c. In which scenarios could CHL be advantageous over backpropagation?  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **11. Explain the relationship between Variational Autoencoders (VAEs) and Bayesian Inference.**  \n",
    "- a. How does the reparameterization trick enable backpropagation in VAEs?  \n",
    "- b. Why is the Evidence Lower Bound (ELBO) critical in VAE training?  \n",
    "- c. Discuss how VAEs handle uncertainty in data generation compared to standard autoencoders.  \n",
    "\n",
    "### **12. Contrast Restricted Boltzmann Machines (RBMs) and Generative Adversarial Networks (GANs).**  \n",
    "- a. How do these models differ in their approach to learning data distributions?  \n",
    "- b. What are the computational challenges unique to each model?  \n",
    "- c. Which scenarios favor RBMs over GANs, and why?  \n",
    "\n",
    "### **13. Discuss the role of cooling schedules in Simulated Annealing-based optimization.**  \n",
    "- a. How does the choice of cooling function impact convergence speed and quality?  \n",
    "- b. Compare linear, exponential, and logarithmic cooling schedules in practical applications.  \n",
    "- c. Provide a real-world scenario where Simulated Annealing could outperform gradient-based methods.  \n",
    "\n",
    "### **14. Explain the importance of feature embeddings in graph neural networks (GNNs).**  \n",
    "- a. How are node embeddings learned in GNNs, and why are they useful?  \n",
    "- b. Discuss the differences between spectral-based and spatial-based GNNs for embedding learning.  \n",
    "- c. How do GNN embeddings contribute to applications like recommendation systems or drug discovery?  \n",
    "\n",
    "### **15. Why is the KL Divergence term in Variational Autoencoders (VAEs) both a strength and a limitation?**  \n",
    "- a. Explain the role of KL divergence in regularizing the latent space.  \n",
    "- b. Discuss the trade-off between reconstruction loss and KL divergence in VAE training.  \n",
    "- c. How do techniques like β-VAEs adjust this trade-off for specific applications?  \n",
    "\n",
    "### **16. Attention Mechanisms in Sequence-to-Sequence Models:**  \n",
    "- a. Explain how attention mechanisms solve the bottleneck problem in traditional encoder-decoder architectures.  \n",
    "- b. Why is self-attention particularly powerful in Transformer models compared to additive or multiplicative attention?  \n",
    "- c. Discuss the computational trade-offs of using attention mechanisms in large-scale sequence modeling tasks.  \n",
    "\n",
    "### **17. Explain how Wasserstein GANs (WGANs) address mode collapse in GAN training.**  \n",
    "- a. Why is the Earth Mover’s Distance (Wasserstein Distance) a better metric than KL or JS divergence for GANs?  \n",
    "- b. What is the role of weight clipping in WGANs, and how does it affect stability?  \n",
    "- c. Discuss the improvements introduced by WGAN-GP over the original WGAN.  \n",
    "\n",
    "### **18. Residual Connections in Deep Neural Networks:**  \n",
    "- a. Why do residual connections mitigate the vanishing gradient problem in deep networks?  \n",
    "- b. Explain how residual blocks enable the training of extremely deep models like ResNet.  \n",
    "- c. Compare the role of residual connections in convolutional networks and transformers.  \n",
    "\n",
    "### **19. Bias-Variance Trade-off in Generative Neural Networks:**  \n",
    "- a. How does high variance manifest in the output of GANs or VAEs?  \n",
    "- b. Provide techniques to control bias and variance during the training of generative models.  \n",
    "- c. Discuss the role of data augmentation in managing bias-variance trade-offs.  \n",
    "\n",
    "### **20. Explain the role of Contrastive Learning in feature representation.**  \n",
    "- a. How does contrastive learning leverage positive and negative pairs to learn embeddings?  \n",
    "- b. Compare supervised contrastive learning and self-supervised contrastive learning.  \n",
    "- c. Discuss how contrastive learning can be applied to improve embeddings for downstream tasks like clustering or classification.  \n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
