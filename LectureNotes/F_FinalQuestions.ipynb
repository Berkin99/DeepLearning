{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **CSE655 Midterm Questions**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **1. Estimated Questions**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. PARAMETER CALCULATION**\n",
    "* Calculate Tunable parameters given CNN\n",
    "* Calculate Tunable parameters given RNN\n",
    "* Calculate Tunable parameters given Autoencoder\n",
    "* Calculate Tunable parameters given U-Net\n",
    "* Calculate Tunable parameters given Res-Net\n",
    "* Calculate Tunable parameters given LSTM\n",
    "* Calculate Tunable parameters given RBM\n",
    "\n",
    "**2. DRAW DIAGRAM**\n",
    "* Draw Transformer Architecture\n",
    "* Draw a GAN Diagram for Image Generation\n",
    "\n",
    "**3. Contrastive Hebbian Learning**\n",
    "* Explain Contrastive Hebbian Learning steps by iteratively.\n",
    "\n",
    "**4. BOLTZMANN**\n",
    "* Calculate Tunable parameters given Restricted Boltzmann Machine\n",
    "* Regularization and optimization techniques for Boltzmann Machine\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **2. CHATGPT Estimated Questions**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1. Explain the difference between Conditional Autoencoders and Variational Autoencoders.**  \n",
    "- a. How does the latent space representation differ between these two architectures?  \n",
    "- b. Provide an example use case where each would be suitable.  \n",
    "- c. How could combining these two architectures (Conditional and Variational) be useful?  \n",
    "\n",
    "### **2. Describe how Simulated Annealing is used in neural network weight optimization.**  \n",
    "- a. What are the advantages of using simulated annealing compared to gradient descent?  \n",
    "- b. How does the cooling schedule affect convergence speed and solution quality?  \n",
    "- c. Give an example of a problem where simulated annealing would outperform standard optimizers.  \n",
    "\n",
    "### **3. Explain the training process of Restricted Boltzmann Machines (RBMs).**  \n",
    "- a. What is the role of the hidden and visible layers in RBMs?  \n",
    "- b. How does Contrastive Divergence approximate the training objective?  \n",
    "- c. Explain how RBMs can be stacked to form Deep Belief Networks (DBNs).  \n",
    "\n",
    "### **4. Compare and contrast the KL Divergence and Wasserstein Distance in generative models.**  \n",
    "- a. Why is KL divergence often used in VAEs but not in GANs?  \n",
    "- b. How does Wasserstein Distance improve GAN training stability?  \n",
    "- c. Which of these metrics would you use for evaluating mode diversity in generative models, and why?  \n",
    "\n",
    "### **5. Describe the importance of Feature Embeddings in Recommendation Systems.**  \n",
    "- a. How are embeddings generated for users and items in collaborative filtering?  \n",
    "- b. Explain how embeddings can be fine-tuned during the recommendation process.  \n",
    "- c. What are the challenges of using pre-trained embeddings for recommendation systems?  \n",
    "\n",
    "### **6. Explain the advantages of Transformers over Recurrent Neural Networks.**  \n",
    "- a. How does self-attention allow Transformers to process sequences more efficiently?  \n",
    "- b. Why do Transformers perform better on long sequences compared to RNNs?  \n",
    "- c. Discuss a real-world example where Transformers outperform traditional sequence models.  \n",
    "\n",
    "### **7. Discuss Regularization Techniques in Generative Neural Networks.**  \n",
    "- a. What is the role of gradient penalty in Wasserstein GANs?  \n",
    "- b. How can spectral normalization improve the discriminator's stability?  \n",
    "- c. Explain why dropout is rarely used in GAN training compared to other regularization methods.  \n",
    "\n",
    "### **8. Calculate the parameters of a Transformer Encoder Block:**  \n",
    "- Input: Sequence length = 128, Embedding dimension = 512  \n",
    "- Multi-Head Attention: 8 heads, Key/Query/Value dimensions = 64  \n",
    "- Feedforward Layers: First layer = 2048 units, Second layer = 512 units  \n",
    "\n",
    "_Hint: Consider parameters for attention layers, linear transformations, and feedforward layers._  \n",
    "\n",
    "### **9. Explain the role of attention mechanisms in Variational Autoencoders.**  \n",
    "- a. How can attention be integrated into the encoder-decoder structure of VAEs?  \n",
    "- b. What are the benefits of using attention for latent variable modeling?  \n",
    "- c. Provide a use case where attention-augmented VAEs would outperform standard VAEs.  \n",
    "\n",
    "### **10. How does Contrastive Hebbian Learning differ from Backpropagation?**  \n",
    "- a. What is the biological motivation behind Contrastive Hebbian Learning?  \n",
    "- b. How does the update rule in CHL approximate gradient descent?  \n",
    "- c. In which scenarios could CHL be advantageous over backpropagation?  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **11. Explain the relationship between Variational Autoencoders (VAEs) and Bayesian Inference.**  \n",
    "- a. How does the reparameterization trick enable backpropagation in VAEs?  \n",
    "- b. Why is the Evidence Lower Bound (ELBO) critical in VAE training?  \n",
    "- c. Discuss how VAEs handle uncertainty in data generation compared to standard autoencoders.  \n",
    "\n",
    "### **12. Contrast Restricted Boltzmann Machines (RBMs) and Generative Adversarial Networks (GANs).**  \n",
    "- a. How do these models differ in their approach to learning data distributions?  \n",
    "- b. What are the computational challenges unique to each model?  \n",
    "- c. Which scenarios favor RBMs over GANs, and why?  \n",
    "\n",
    "### **13. Discuss the role of cooling schedules in Simulated Annealing-based optimization.**  \n",
    "- a. How does the choice of cooling function impact convergence speed and quality?  \n",
    "- b. Compare linear, exponential, and logarithmic cooling schedules in practical applications.  \n",
    "- c. Provide a real-world scenario where Simulated Annealing could outperform gradient-based methods.  \n",
    "\n",
    "### **14. Explain the importance of feature embeddings in graph neural networks (GNNs).**  \n",
    "- a. How are node embeddings learned in GNNs, and why are they useful?  \n",
    "- b. Discuss the differences between spectral-based and spatial-based GNNs for embedding learning.  \n",
    "- c. How do GNN embeddings contribute to applications like recommendation systems or drug discovery?  \n",
    "\n",
    "### **15. Why is the KL Divergence term in Variational Autoencoders (VAEs) both a strength and a limitation?**  \n",
    "- a. Explain the role of KL divergence in regularizing the latent space.  \n",
    "- b. Discuss the trade-off between reconstruction loss and KL divergence in VAE training.  \n",
    "- c. How do techniques like β-VAEs adjust this trade-off for specific applications?  \n",
    "\n",
    "### **16. Attention Mechanisms in Sequence-to-Sequence Models:**  \n",
    "- a. Explain how attention mechanisms solve the bottleneck problem in traditional encoder-decoder architectures.  \n",
    "- b. Why is self-attention particularly powerful in Transformer models compared to additive or multiplicative attention?  \n",
    "- c. Discuss the computational trade-offs of using attention mechanisms in large-scale sequence modeling tasks.  \n",
    "\n",
    "### **17. Explain how Wasserstein GANs (WGANs) address mode collapse in GAN training.**  \n",
    "- a. Why is the Earth Mover’s Distance (Wasserstein Distance) a better metric than KL or JS divergence for GANs?  \n",
    "- b. What is the role of weight clipping in WGANs, and how does it affect stability?  \n",
    "- c. Discuss the improvements introduced by WGAN-GP over the original WGAN.  \n",
    "\n",
    "### **18. Residual Connections in Deep Neural Networks:**  \n",
    "- a. Why do residual connections mitigate the vanishing gradient problem in deep networks?  \n",
    "- b. Explain how residual blocks enable the training of extremely deep models like ResNet.  \n",
    "- c. Compare the role of residual connections in convolutional networks and transformers.  \n",
    "\n",
    "### **19. Bias-Variance Trade-off in Generative Neural Networks:**  \n",
    "- a. How does high variance manifest in the output of GANs or VAEs?  \n",
    "- b. Provide techniques to control bias and variance during the training of generative models.  \n",
    "- c. Discuss the role of data augmentation in managing bias-variance trade-offs.  \n",
    "\n",
    "### **20. Explain the role of Contrastive Learning in feature representation.**  \n",
    "- a. How does contrastive learning leverage positive and negative pairs to learn embeddings?  \n",
    "- b. Compare supervised contrastive learning and self-supervised contrastive learning.  \n",
    "- c. Discuss how contrastive learning can be applied to improve embeddings for downstream tasks like clustering or classification.  \n",
    "\n",
    "### **21. Calculate the tunable parameters in this Variational Autoencoder (VAE):**  \n",
    "- Encoder:  \n",
    "    - Input: 64x64x3  \n",
    "    - 3x3 Conv 16  \n",
    "    - 3x3 Conv 32  \n",
    "    - Latent Layer: 128  \n",
    "- Decoder:  \n",
    "    - Latent Layer: 128  \n",
    "    - Dense: 1024  \n",
    "    - Output: 64x64x3  \n",
    "\n",
    "_Hint: Include the parameters for mean and variance in the latent space._  \n",
    "\n",
    "### **22. Describe how Conditional GANs (cGANs) work.**  \n",
    "- a. What additional input is provided to the generator and discriminator?  \n",
    "- b. How do cGANs differ from standard GANs in terms of data generation?  \n",
    "- c. Give an example use case of cGANs and explain why they are effective in that scenario.  \n",
    "\n",
    "### **23. Explain the role of the cooling schedule in Simulated Annealing.**  \n",
    "- a. What is the purpose of the cooling schedule?  \n",
    "- b. Define the trade-off between exploration and exploitation in simulated annealing.  \n",
    "- c. Give an example of how you would design a cooling schedule for optimizing weights in a neural network.  \n",
    "\n",
    "### **24. Compare Contrastive Divergence and Contrastive Hebbian Learning.**  \n",
    "- a. Explain the differences in their training objectives.  \n",
    "- b. Discuss scenarios where one might be preferred over the other.  \n",
    "- c. How does Contrastive Divergence address the computational challenges of RBMs?  \n",
    "\n",
    "### **25. For Attention Blocks in Transformers:**  \n",
    "- a. Calculate the total number of parameters in a multi-head attention layer with 8 heads, input dimension = 64, and output dimension = 128.  \n",
    "- b. Why is scaling applied in scaled dot-product attention?  \n",
    "- c. Explain how positional encoding helps attention blocks understand sequential data.  \n",
    "\n",
    "### **26. Loss Functions for GANs:**  \n",
    "- a. Explain why standard GANs often suffer from instability during training.  \n",
    "- b. Describe how Wasserstein Loss addresses these issues.  \n",
    "- c. Discuss the impact of gradient penalty in WGAN-GP.  \n",
    "\n",
    "### **27. Explain the role of Feature Embeddings in Natural Language Processing (NLP):**  \n",
    "- a. What is the difference between a one-hot vector and a dense embedding vector?  \n",
    "- b. Why are dense embedding vectors more efficient for representing semantic relationships?  \n",
    "- c. Provide an example of how embeddings can be used in a recommendation system.  \n",
    "\n",
    "### **28. Discuss Variational Autoencoders (VAEs):**  \n",
    "- a. Explain the difference between the encoder output and the latent space representation in a VAE.  \n",
    "- b. Why do we use the KL divergence term in the VAE loss function?  \n",
    "- c. How would you modify a VAE to handle image data with high resolution (e.g., 1024x1024)?  \n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
