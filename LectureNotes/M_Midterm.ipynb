{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **CSE655 Midterm**\n",
    "\n",
    "* **Perceptron**\n",
    "* **Loss Function**\n",
    "* **Activation Functions (Sigmoid, ReLU, Tanh ...)**\n",
    "* **Feed Forward Networks**\n",
    "* **Gradient Descent**\n",
    "* **Stochastic Gradient Descent (SGD)**\n",
    "* **Mini-batch Gradient Descent**\n",
    "* **Back Propagation**\n",
    "* **Optimization**\n",
    "* **Regularization**\n",
    "* **Vanishing and Exploding Gradients**\n",
    "* **Bias-Variance Tradeoff**\n",
    "* **Overfitting and Underfitting**\n",
    "* **Learning Rate**\n",
    "* **Cross-Entropy Loss**\n",
    "* **Softmax Function**\n",
    "* **Convolutional Neural Networks (CNN)**\n",
    "* **Batch Normalization**\n",
    "\n",
    "## **Foundational Concepts**\n",
    "- **Supervised vs. Unsupervised Learning**\n",
    "- **Perceptron and Multilayer Perceptrons (MLP)**\n",
    "- **Loss Functions**:\n",
    "  - Mean Squared Error (MSE)\n",
    "  - Cross-Entropy Loss\n",
    "- **Activation Functions**:\n",
    "  - Sigmoid, ReLU, Tanh\n",
    "  - Properties and Comparison\n",
    "- **Feedforward Neural Networks**:\n",
    "  - Forward Pass\n",
    "  - Computation Graphs\n",
    "\n",
    "---\n",
    "\n",
    "## **Optimization Techniques**\n",
    "- **Gradient Descent Variants**:\n",
    "  - Batch Gradient Descent\n",
    "  - Stochastic Gradient Descent (SGD)\n",
    "  - Mini-batch Gradient Descent\n",
    "- **Backpropagation Algorithm**\n",
    "- **Learning Rate**:\n",
    "  - Learning Rate Schedules\n",
    "  - Effects of Large/Small Learning Rates\n",
    "- **Momentum-Based Optimizers**:\n",
    "  - Adam, RMSprop, Adagrad\n",
    "\n",
    "---\n",
    "\n",
    "## **Model Challenges and Improvements**\n",
    "- **Overfitting and Underfitting**\n",
    "  - Techniques to Mitigate Overfitting:\n",
    "    - Dropout\n",
    "    - L1/L2 Regularization\n",
    "    - Early Stopping\n",
    "- **Bias-Variance Tradeoff**\n",
    "- **Vanishing and Exploding Gradients**\n",
    "  - Role of Activation Functions\n",
    "  - Gradient Clipping\n",
    "\n",
    "---\n",
    "\n",
    "## **Advanced Topics**\n",
    "- **Softmax Function and its Use in Classification**\n",
    "- **Cross-Entropy for Multi-Class Problems**\n",
    "- **Regularization Techniques**\n",
    "  - Weight Decay\n",
    "  - Data Augmentation\n",
    "- **Batch Normalization**:\n",
    "  - Benefits and Drawbacks\n",
    "  - Impact on Training Speed\n",
    "\n",
    "---\n",
    "\n",
    "## **Deep Architectures**\n",
    "- **Convolutional Neural Networks (CNNs)**:\n",
    "  - Convolutional Layers\n",
    "  - Pooling Layers\n",
    "  - Dropout Layers\n",
    "  - Applications of CNNs\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
