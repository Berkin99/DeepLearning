{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **CSE655 Final**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1. AutoEncoders (Conditional, Variational, Conditional and Variational)**  \n",
    "\n",
    "1. **Autoencoder**: Girdi verilerini sıkıştırarak bir temsil (encoding) oluşturur ve bunu yeniden oluşturur.  \n",
    "2. **Latent Space**: Verilerin gizli temsil edildiği sıkıştırılmış vektör alanıdır.  \n",
    "3. **Reconstruction Loss**: Autoencoder’ların çıktı ile girdiyi ne kadar iyi eşleştirdiğini ölçen kayıp fonksiyonu.  \n",
    "4. **Conditional Autoencoder**: Ek bilgi (etiket gibi) kullanarak hedefli veri yeniden inşası yapar.  \n",
    "5. **Variational Autoencoder (VAE)**: Latent space’in bir olasılık dağılımını öğrenmesini sağlar.  \n",
    "6. **KL Divergence**: VAE’lerde kullanılan, dağılımlar arasındaki farkı ölçen bir kayıp metriği.  \n",
    "7. **Decoder**: Latent space’ten veriyi yeniden oluşturan model bölümü.  \n",
    "8. **Encoder**: Girdileri latent space’e dönüştüren model bölümü.  \n",
    "9. **Reparameterization Trick**: VAE’nin öğrenim sürecinde gradyan hesaplamasını kolaylaştırır.  \n",
    "10. **Regularization in VAE**: Latent space’i düzenleyerek daha anlamlı dağılımlar oluşturur.  \n",
    "11. **Denoising Autoencoder**: Gürültülü verilerden temiz veri öğrenmeyi amaçlayan özel bir Autoencoder türü.  \n",
    "12. **Sparse Autoencoder**: Daha seyrek özellik temsilleri elde etmek için düzenleme ekler.  \n",
    "13. **Overfitting in Autoencoders**: Latent space’in aşırı karmaşık hale gelmesiyle oluşur.  \n",
    "14. **Generative Autoencoders**: Yeni veri örnekleri oluşturabilen Autoencoder türü.  \n",
    "15. **Input Reconstruction**: Autoencoder’ın temel çıktısı olan yeniden yapılandırılmış girdiler.  \n",
    "16. **Anomaly Detection**: Autoencoders kullanılarak anormal verilerin tespiti.  \n",
    "17. **Latent Space Interpolation**: Latent vektörler arasında yumuşak geçişler oluşturma.  \n",
    "18. **Beta-VAE**: Latent temsillerin daha açıklanabilir hale gelmesi için VAE’nin bir türevi.  \n",
    "19. **Reconstruction vs. Generation**: Autoencoders’ın girdi üretme ile yeniden yapılandırma arasındaki farkı.  \n",
    "20. **Conditional Generation**: Belirli koşullara dayalı veri üretimi (örneğin, sınıf etiketi ile).  \n",
    "\n",
    "### **2. Feature Embeddings**  \n",
    "\n",
    "1. **Embedding Space**: Verilerin düşük boyutlu sürekli bir vektör uzayında temsil edilmesi.  \n",
    "2. **Word Embeddings**: Kelimeleri vektör olarak temsil eden yöntemler (örneğin, Word2Vec, GloVe).  \n",
    "3. **Learnable Embeddings**: Model tarafından eğitim sırasında öğrenilen vektör temsilleri.  \n",
    "4. **Dimensionality Reduction**: Özellik uzayını daha az boyutlu hale getirme süreci (örneğin, PCA).  \n",
    "5. **Semantic Similarity**: Embedding’lerde vektörler arasındaki benzerlik, anlam ilişkilerini yansıtır.  \n",
    "6. **One-Hot Encoding**: Kategorik veriyi yüksek boyutlu, seyrek bir vektörle temsil etme yöntemi.  \n",
    "7. **Dense Embeddings**: One-hot encoding’in aksine, verilerin sıkıştırılmış bir temsili.  \n",
    "8. **Embedding Layer**: Neural network’te giriş verilerini vektör uzayına dönüştüren katman.  \n",
    "9. **Pretrained Embeddings**: Önceden eğitilmiş vektör temsilleri (örneğin, FastText, BERT).  \n",
    "10. **Transfer Learning**: Başka bir problemde eğitilmiş embedding’leri yeni bir görevde kullanma.  \n",
    "11. **Contextualized Embeddings**: Kelimenin bağlama göre farklı vektörle temsil edilmesi (örneğin, BERT).  \n",
    "12. **T-SNE**: Embedding vektörlerini görselleştirmek için kullanılan boyut azaltma yöntemi.  \n",
    "13. **Cosine Similarity**: Embedding’ler arasındaki açısal benzerliği ölçen bir metrik.  \n",
    "14. **Euclidean Distance**: Embedding vektörleri arasındaki doğrusal uzaklık ölçümü.  \n",
    "15. **Application in NLP**: Metin sınıflandırma, dil modelleme, çeviri gibi NLP uygulamaları.  \n",
    "16. **Negative Sampling**: Word2Vec gibi yöntemlerde kullanılan, eğitim sürecini hızlandıran teknik.  \n",
    "17. **Positional Embeddings**: Transformer’larda dizisel sıralama bilgisi eklemek için kullanılır.  \n",
    "18. **Cold Start Problem**: Yeni kullanıcı/öğe verisi olmadığında öneri sistemlerinde yaşanan sorun, embedding’lerle çözülür.  \n",
    "19. **Continuous Representations**: Kategorik verilerin sürekli bir uzayda temsili.  \n",
    "20. **Feature Engineering**: Özelliklerin embedding'lerle öğrenildiği, el ile seçime gerek kalmayan süreç.  \n",
    "\n",
    "### **3. Boltzmann Machine**  \n",
    "\n",
    "1. **Boltzmann Machine**: Enerji bazlı, olasılıksal bir yapay sinir ağı modeli.  \n",
    "2. **Stochastic Neurons**: Çıkışları olasılık dağılımına göre belirlenen nöronlar.  \n",
    "3. **Energy Function**: Sistem durumlarının maliyetini hesaplayan fonksiyon.  \n",
    "4. **Hidden Units**: Gözlemlenemeyen, öğrenilmiş özellikleri temsil eden nöronlar.  \n",
    "5. **Visible Units**: Gözlemlenebilir veriyle doğrudan bağlantılı nöronlar.  \n",
    "6. **Weight Matrix**: Nöronlar arasındaki bağlantıları temsil eden ağırlıklar.  \n",
    "7. **Symmetric Weights**: Boltzmann Machine’de bağlantıların çift yönlü olması.  \n",
    "8. **Energy Minimization**: Modelin, veri dağılımını öğrenmek için enerji fonksiyonunu minimize etmesi.  \n",
    "9. **Contrastive Divergence**: Öğrenme sürecini hızlandıran bir optimizasyon yöntemi.  \n",
    "10. **Sampling**: Enerji fonksiyonundan olasılık örnekleri oluşturma süreci.  \n",
    "11. **Restricted Boltzmann Machine (RBM)**: Tam bağlantılı bir yapının kısıtlanmış versiyonu.  \n",
    "12. **Free Energy**: Modelin farklı durumlarını açıklamak için kullanılan metrik.  \n",
    "13. **Markov Chain**: Boltzmann Machine’in durumlar arasında geçiş yapmak için kullandığı süreç.  \n",
    "14. **Annealing**: Daha iyi sonuçlar elde etmek için sıcaklığın kademeli olarak azaltılması.  \n",
    "15. **Partition Function**: Enerji durumlarının toplam olasılığını normalleştiren fonksiyon.  \n",
    "16. **Log-Likelihood Maximization**: Verilerin olasılığını en üst düzeye çıkarmayı hedefler.  \n",
    "17. **Binary States**: Boltzmann Machine’deki nöronların iki durumu (0 veya 1).  \n",
    "18. **Gibbs Sampling**: Nöron durumlarını güncellemek için kullanılan bir algoritma.  \n",
    "19. **Energy-Based Models**: Boltzmann Machine’in bağlı olduğu geniş model sınıfı.  \n",
    "20. **Unsupervised Learning**: Boltzmann Machine’in verilerden etiket olmadan öğrenmesi.  \n",
    "\n",
    "### **4. Simulated Annealing (Cooling Schedule)**  \n",
    "\n",
    "1. **Simulated Annealing**: Optimizasyon problemleri için kullanılan sezgisel bir algoritma.  \n",
    "2. **Cooling Schedule**: Sıcaklık parametresinin azaltılma stratejisi.  \n",
    "3. **Global Optimization**: Yerel minimumlardan kaçınarak en iyi çözümü bulma.  \n",
    "4. **Temperature Parameter**: Çözüm arama sürecinde rastgeleliğin derecesini belirler.  \n",
    "5. **Exploration vs. Exploitation**: Yeni bölgeleri keşfetme ile mevcut çözümleri iyileştirme dengesi.  \n",
    "6. **Annealing Process**: Metalleri yavaşça soğutma prensibinden ilham alır.  \n",
    "7. **Energy Landscape**: Çözüm uzayındaki enerji seviyelerini temsil eder.  \n",
    "8. **Probability of Acceptance**: Daha kötü çözümlerin kabul edilme olasılığı.  \n",
    "9. **Initial Temperature**: Rastgeleliği başlatmak için kullanılan başlangıç değeri.  \n",
    "10. **Cooling Rate**: Sıcaklığın ne hızla azaltılacağını belirler.  \n",
    "11. **Metropolis Criterion**: Kötü çözümleri kabul etmek için kullanılan karar kriteri.  \n",
    "12. **Stochastic Process**: Simulated Annealing’in rastgeleliğe dayalı yapısı.  \n",
    "13. **Local Minima**: Global çözümün önüne geçebilecek düşük enerji seviyeleri.  \n",
    "14. **Boltzmann Distribution**: Enerji bazlı sistemlerin durum olasılıklarını belirler.  \n",
    "15. **Optimization Problems**: Simulated Annealing’in uygulandığı geniş problem sınıfı.  \n",
    "16. **Combinatorial Optimization**: Ayrık problemlerin çözümünde kullanımı.  \n",
    "17. **Noisy Environments**: Gürültülü veri setlerinde çözüm bulma yeteneği.  \n",
    "18. **Heuristics**: Simulated Annealing’in problem çözme stratejisi.  \n",
    "19. **Acceptance Probability Function**: Sıcaklık ve enerji farkına göre hesaplanır.  \n",
    "20. **Stopping Criteria**: Algoritmanın ne zaman duracağını belirleyen koşul.  \n",
    "\n",
    "### **5. Contrastive Hebbian Learning (CHL)**  \n",
    "\n",
    "1. **Contrastive Hebbian Learning**: Hebbian öğrenmeye dayalı, enerji tabanlı bir öğrenme algoritması.  \n",
    "2. **Positive Phase**: Gözlemlenen verilerle modelin enerji minimizasyonu yaptığı aşama.  \n",
    "3. **Negative Phase**: Modelin yalnızca öğrenilmiş bağlantılarla serbest bırakıldığı aşama.  \n",
    "4. **Energy Function**: CHL’deki durumlar arasındaki enerji seviyelerini hesaplayan fonksiyon.  \n",
    "5. **Reconstruction Error**: Gözlemlenen ve üretilen veriler arasındaki fark.  \n",
    "6. **Hebbian Rule**: Nöronlar eşzamanlı ateşlendiğinde bağlantılarını güçlendiren kural.  \n",
    "7. **Anti-Hebbian Rule**: Nöronlar farklı zamanlarda ateşlendiğinde bağlantılarını zayıflatan kural.  \n",
    "8. **Iterative Updates**: CHL algoritmasının ağırlıkları kademeli olarak güncellemesi.  \n",
    "9. **Symmetric Weights**: CHL’de bağlantıların çift yönlü olması.  \n",
    "10. **Convergence**: Pozitif ve negatif fazların dengelendiği nokta.  \n",
    "11. **Energy Gradient**: Enerji fonksiyonunun öğrenmeyi yönlendirdiği eğim.  \n",
    "12. **Correlational Learning**: Nöron aktivasyonları arasındaki ilişkiyi öğrenme.  \n",
    "13. **Dynamic Equilibrium**: Pozitif ve negatif fazlar arasında denge sağlanması.  \n",
    "14. **Associative Memory**: CHL ile öğrenilen bilgilerin hafızada saklanması.  \n",
    "15. **Unsupervised Learning**: CHL’nin etiket olmadan öğrenim yapabilmesi.  \n",
    "16. **Contrastive Divergence**: CHL’ye benzer şekilde pozitif ve negatif fazları kullanır.  \n",
    "17. **Sparse Representations**: CHL’nin seyrek veri temsillerine yönlendirme kapasitesi.  \n",
    "18. **Network Stability**: CHL’nin öğrenim sırasında dengesini koruması.  \n",
    "19. **Learning Rate**: Ağırlıkların güncellenme hızını kontrol eden parametre.  \n",
    "20. **Energy Minimization**: CHL’nin nihai amacı olan enerji düşüşü.  \n",
    "\n",
    "### **6. Restricted Boltzmann Machines (RBM)**  \n",
    "\n",
    "1. **RBM Architecture**: Gizli ve görünür birimlerden oluşan iki katmanlı yapı.  \n",
    "2. **Binary Hidden Units**: RBM’nin gizli katmanındaki ikili durumlar.  \n",
    "3. **Visible Units**: Gözlemlenebilir verilerle bağlantılı giriş katmanı.  \n",
    "4. **Energy-Based Model**: RBM’nin enerji minimizasyonuna dayalı modeli.  \n",
    "5. **Contrastive Divergence**: RBM’lerin eğitimini hızlandırmak için kullanılan algoritma.  \n",
    "6. **Weight Matrix**: Görünür ve gizli katmanlar arasındaki bağlantıların ağırlıkları.  \n",
    "7. **Sampling**: RBM’nin veri noktalarını enerji dağılımından seçmesi.  \n",
    "8. **Hidden Representations**: Verilerin gizli katmandaki temsili.  \n",
    "9. **Probability Estimation**: Gizli ve görünür birimlerin durum olasılıklarının hesaplanması.  \n",
    "10. **Partition Function**: Modelin olasılıklarını normalize eden fonksiyon.  \n",
    "11. **Free Energy**: RBM’nin enerji seviyelerinin bir ölçüsü.  \n",
    "12. **Gibbs Sampling**: RBM’de kullanılan bir örnekleme algoritması.  \n",
    "13. **Deep Belief Networks**: RBM’lerin katmanlı bir yapı oluşturmasıyla oluşan model.  \n",
    "14. **Unsupervised Pretraining**: RBM’nin etiketsiz verilerle eğitim yapması.  \n",
    "15. **Binary States**: RBM’nin birimlerinde 0 veya 1 durumu.  \n",
    "16. **Activation Function**: Gizli birimlerin aktivasyonlarını belirler.  \n",
    "17. **Overfitting Prevention**: RBM’nin düzenleme teknikleriyle aşırı öğrenimi önlemesi.  \n",
    "18. **Training Epochs**: Eğitim sürecinde kullanılan iterasyon sayısı.  \n",
    "19. **Data Reconstruction**: Görünür birimlerden girdi verilerini yeniden oluşturma.  \n",
    "20. **Weight Sharing**: Ağırlıkların simetrik olması ve çift yönlü bilgi akışı sağlaması.  \n",
    "\n",
    "### **7. Deep Belief Networks (DBNs)**  \n",
    "\n",
    "1. **Layer-Wise Training**: DBN’lerde her katmanın ayrı ayrı eğitilmesi.  \n",
    "2. **Stacked RBMs**: RBM’lerin ardışık şekilde birleştirilmesiyle oluşan yapı.  \n",
    "3. **Greedy Learning**: DBN’lerde katman bazlı ön eğitim yöntemi.  \n",
    "4. **Unsupervised Pretraining**: Verilerin etiketsiz olarak öğrenildiği ilk adım.  \n",
    "5. **Fine-Tuning**: DBN’nin son katmanlarının etiketli verilerle eğitimi.  \n",
    "6. **Hierarchical Feature Learning**: DBN’nin katmanlarda farklı seviyelerde özellikler öğrenmesi.  \n",
    "7. **Restricted Boltzmann Machine**: DBN’nin temel yapı taşı.  \n",
    "8. **Generative Model**: Verilerin olasılıksal yapısını modelleyen DBN yaklaşımı.  \n",
    "9. **Feature Abstraction**: DBN’nin veri temsillerini daha soyut hale getirmesi.  \n",
    "10. **Energy-Based Learning**: DBN’nin enerji minimizasyonuna dayalı öğrenmesi.  \n",
    "11. **Hidden Layer Representations**: DBN’nin gizli katmanlarda veri özelliklerini öğrenmesi.  \n",
    "12. **Graphical Models**: DBN’nin olasılıksal grafik modellerle ilişkilendirilmesi.  \n",
    "13. **Probabilistic Generative Networks**: DBN’nin olasılıksal veri üretme kapasitesi.  \n",
    "14. **Overfitting Control**: Aşırı öğrenimi önlemek için kullanılan düzenleme teknikleri.  \n",
    "15. **Gradient Descent**: Ağırlıkları optimize etmek için kullanılan algoritma.  \n",
    "16. **Backpropagation**: DBN’nin son katmanlarını ince ayarlamak için kullanılan teknik.  \n",
    "17. **Model Depth**: DBN’nin katman sayısının artırılmasıyla elde edilen derinlik.  \n",
    "18. **Parameter Sharing**: Katmanlardaki bağlantıların ağırlıklarının paylaşımı.  \n",
    "19. **Reconstruction Error**: Verilerin yeniden yapılandırılması sırasında oluşan hata.  \n",
    "20. **Multilayer Perceptrons (MLPs)**: DBN’nin son aşamasında kullanılan tam bağlantılı yapılar.  \n",
    "\n",
    "### **8. Generative Neural Networks**  \n",
    "\n",
    "1. **Generative Models**: Veri dağılımını öğrenerek yeni veri örnekleri oluşturan modeller.  \n",
    "2. **Latent Space**: Yeni veri oluşturmak için kullanılan sıkıştırılmış temsillerin bulunduğu uzay.  \n",
    "3. **Decoder Networks**: Latent temsilleri yeniden veri formatına dönüştüren ağlar.  \n",
    "4. **Probabilistic Models**: Verilerin olasılık dağılımını modelleyen yöntemler.  \n",
    "5. **Variational Autoencoders (VAEs)**: Latent uzayı düzenleyen, olasılıksal bir generative model türü.  \n",
    "6. **Energy-Based Models**: Enerji minimizasyonu ile veri üreten modeller.  \n",
    "7. **Bayesian Networks**: Generative süreçte olasılıksal ilişkileri modelleyen yapılar.  \n",
    "8. **Normalizing Flows**: Karmaşık veri dağılımlarını modelleyen invertibl dönüşümler.  \n",
    "9. **Sample Generation**: Öğrenilen dağılımdan yeni veri örnekleri üretme süreci.  \n",
    "10. **Training Stability**: Generative modellerde eğitim sırasında kararlılık sağlama.\n",
    "\n",
    "### **9. GANs (Generative Adversarial Networks)**  \n",
    "\n",
    "1. **Generator**: Rastgele gürültüyü gerçekçi veri oluşturacak şekilde dönüştüren model.  \n",
    "2. **Discriminator**: Gerçek ve sahte veriyi ayırt etmeye çalışan model.  \n",
    "3. **Adversarial Loss**: Generator ve discriminator’un birbirine karşı yarışmasını sağlayan kayıp fonksiyonu.  \n",
    "4. **Minimax Optimization**: GAN’lerde iki modelin karşıt hedeflere ulaşmaya çalıştığı optimizasyon süreci.  \n",
    "5. **Mode Collapse**: Generator’un sınırlı bir veri türü üretmesi durumu.  \n",
    "6. **Conditional GANs (cGANs)**: Veri üretimini belirli koşullara dayandıran GAN türü.  \n",
    "7. **Wasserstein GAN (WGAN)**: Eğitim kararlılığını artırmak için kullanılan alternatif bir GAN türü.  \n",
    "8. **Latent Vector**: Generator’un veri üretimi için kullandığı düşük boyutlu giriş.  \n",
    "9. **Unsupervised Learning**: GAN’lerin etiketsiz veriyle öğrenim yapması.  \n",
    "10. **Divergence Metrics**: Generator ve gerçek veri arasındaki farkı ölçen metrikler (örneğin, JS Divergence).\n",
    "\n",
    "### **10. Loss Functions for GANs**  \n",
    "\n",
    "1. **Binary Cross-Entropy Loss**: Discriminator’ın doğru sınıflandırma yapması için kullanılan temel kayıp fonksiyonu.  \n",
    "2. **Adversarial Loss**: Generator’un discriminator’ı yanıltması için kullanılan kayıp.  \n",
    "3. **Least Squares Loss**: GAN eğitimini stabilize eden alternatif bir kayıp fonksiyonu.  \n",
    "4. **Wasserstein Loss**: Veri dağılımları arasındaki mesafeyi minimize eden kayıp.  \n",
    "5. **Hinge Loss**: Discriminator’un daha keskin kararlar vermesini sağlayan bir kayıp türü.  \n",
    "6. **Feature Matching Loss**: Generator’un yalnızca discriminator’un ara katman özelliklerine odaklanmasını sağlayan kayıp.  \n",
    "7. **Perceptual Loss**: Görüntülerin görsel kalitesini iyileştiren bir kayıp fonksiyonu.  \n",
    "8. **KL Divergence**: Olasılık dağılımlarının benzerliğini ölçmek için kullanılan metrik.  \n",
    "9. **Gradient Penalty**: Discriminator’un gradyanlarının kontrol edilmesini sağlayan düzenleme terimi.  \n",
    "10. **Mode Regularization**: Generator’un tüm veri dağılımını kapsamasını teşvik eden kayıp.  \n",
    "\n",
    "### **11. Recurrent Neural Networks (RNNs)**  \n",
    "\n",
    "1. **Sequential Data**: Zaman serisi veya sıralı veriler üzerinde çalışan veri türü.  \n",
    "2. **Hidden States**: RNN’lerin geçmiş bilgiyi sakladığı iç temsil.  \n",
    "3. **Vanishing Gradient Problem**: Uzun dizilerde gradyanların kaybolması sorunu.  \n",
    "4. **Long-Term Dependencies**: Uzun sıralı verilerdeki ilişkileri öğrenme yeteneği.  \n",
    "5. **Time Step**: RNN’lerin her adımda bir veri parçasını işlemesi.  \n",
    "6. **Backpropagation Through Time (BPTT)**: RNN’lerde gradyanların zaman boyunca geriye yayılması.  \n",
    "7. **Unrolling**: RNN’nin zaman adımlarına açılarak eğitim yapılması.  \n",
    "8. **Bidirectional RNN**: Veriyi hem ileri hem de geri yönde işleyen RNN türü.  \n",
    "9. **Gradient Clipping**: Patlayan gradyanları kontrol etmek için kullanılan yöntem.  \n",
    "10. **Context Vector**: Tüm sekans bilgisini özetleyen tek bir vektör.  \n",
    "\n",
    "### **12. LSTM (Long Short-Term Memory)**  \n",
    "\n",
    "1. **Forget Gate**: Hangi bilgilerin unutulacağını belirleyen mekanizma.  \n",
    "2. **Input Gate**: Yeni bilginin hücre durumuna ne kadar katkıda bulunacağını belirler.  \n",
    "3. **Cell State**: Uzun vadeli bilgileri depolayan ana bellek bileşeni.  \n",
    "4. **Output Gate**: Hangi bilginin sonraki aşamaya aktarılacağını kontrol eder.  \n",
    "5. **Vanishing Gradient Solution**: LSTM’nin uzun dizilerde bu sorunu çözme kabiliyeti.  \n",
    "6. **Peephole Connections**: Kapılara hücre durumundan ek bilgi sağlama bağlantıları.  \n",
    "7. **Sequence Modeling**: LSTM’nin sıralı veri işleme yeteneği.  \n",
    "8. **Bi-LSTM**: Sekansları çift yönlü işleyen bir LSTM türü.  \n",
    "9. **Memory Cell**: LSTM’nin temel bilgi saklama birimi.  \n",
    "10. **Temporal Dependencies**: Zaman içindeki ilişkileri öğrenme kabiliyeti.  \n",
    "\n",
    "### **13. Attention Block**  \n",
    "\n",
    "1. **Attention Mechanism**: Girdi dizisinin belirli kısımlarına odaklanan yapı.  \n",
    "2. **Query**: Hangi bilginin aranacağını belirten vektör.  \n",
    "3. **Key**: Bilgi dizisindeki her bir elemanın temsil edildiği vektör.  \n",
    "4. **Value**: Dizinin içeriğini taşıyan vektör.  \n",
    "5. **Scaled Dot-Product Attention**: Query ve Key arasındaki ilişkiyi normalize eden hesaplama.  \n",
    "6. **Self-Attention**: Dizinin kendi içinde hangi kısımların daha önemli olduğunu belirleme.  \n",
    "7. **Context Vector**: Dizi içindeki ilişkileri özetleyen çıktı vektörü.  \n",
    "8. **Multi-Head Attention**: Farklı dikkat bölgelerini paralel olarak modelleme yöntemi.  \n",
    "9. **Softmax Function**: Dikkat skorlarını normalize eden fonksiyon.  \n",
    "10. **Alignment Scores**: Hangi kısımların ne kadar önemli olduğunu belirleyen değerler.  \n",
    "\n",
    "### **14. Transformers**  \n",
    "\n",
    "1. **Encoder-Decoder Architecture**: Transformer’ın temel yapısı, veriyi kodlama ve çözme süreçleri.  \n",
    "2. **Positional Encoding**: Dizideki sıralama bilgisini eklemek için kullanılan yöntem.  \n",
    "3. **Multi-Head Attention**: Dikkati farklı perspektiflerden analiz eden mekanizma.  \n",
    "4. **Feedforward Layers**: Transformer’ın her katmanında kullanılan tam bağlantılı katmanlar.  \n",
    "5. **Layer Normalization**: Eğitim kararlılığını artıran düzenleme yöntemi.  \n",
    "6. **Residual Connections**: Bilginin kaybolmasını önlemek için kullanılan kısa yollar.  \n",
    "7. **Transformer Block**: Encoder veya decoder’daki temel yapı taşı.  \n",
    "8. **Self-Attention Mechanism**: Her bir girdinin diğerleriyle ilişkisini öğrenme.  \n",
    "9. **Tokenization**: Metni küçük parçalara ayırarak modele giriş olarak sağlama.  \n",
    "10. **Pretrained Models**: Transformer tabanlı modellerin (örneğin, BERT, GPT) önceden eğitilmesi.  "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
