{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **4. Optimization**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **4.1 Empirical Risk Minimization (ERM)**\n",
    "\n",
    "Empirical Risk Minimization (ERM) is a fundamental concept in machine learning and deep learning optimization. It is a strategy used to find a model that minimizes the prediction error on a given training dataset. Hereâ€™s a brief breakdown of what ERM entails:\n",
    "\n",
    "- **Risk Minimization**: The goal of machine learning is to find a model that minimizes the expected prediction error (or \"risk\") over the data distribution. This is often called the **true risk** or **expected risk**.\n",
    "\n",
    "- **Empirical Risk**: Since the true data distribution is usually unknown, we approximate the expected risk using the **empirical risk**, which is the average loss calculated over the available training dataset. The empirical risk is the measure of how well a model fits the training data.\n",
    "\n",
    "- **Minimization Process**: ERM seeks to find model parameters $ \\theta $ that minimize the empirical risk. The mathematical formulation is:\n",
    "\n",
    "  $$\n",
    "  \\hat{\\theta} = \\arg \\min_{\\theta} \\frac{1}{n} \\sum_{i=1}^{n} L(y_i, f(x_i; \\theta))\n",
    "  $$\n",
    "\n",
    "  where:\n",
    "  - $ n $ is the number of training examples.\n",
    "  - $ L $ is the loss function measuring the difference between the predicted value $ f(x_i; \\theta) $ and the actual value $ y_i $.\n",
    "  - $ f(x_i; \\theta) $ is the model's prediction for input $ x_i $ given parameters $ \\theta $.\n",
    "  - $ \\hat{\\theta} $ are the optimized parameters that minimize the empirical risk.\n",
    "\n",
    "In deep learning, this minimization is performed using **optimization algorithms** like Gradient Descent, which iteratively adjust the model's weights to reduce the empirical risk.\n",
    "\n",
    "ERM plays a crucial role in training neural networks, as the training process essentially revolves around minimizing the empirical risk to achieve better model performance on unseen data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **4.2 Surrogate Loss Functions**\n",
    "\n",
    "In machine learning, **Surrogate Loss Functions** are alternative loss functions used to make optimization easier or more feasible, particularly when the original problem is difficult to solve directly. Here's a concise overview:\n",
    "\n",
    "- **Purpose**: Surrogate loss functions provide a differentiable and continuous approximation of a non-differentiable or discrete target loss function. They allow for the use of gradient-based optimization techniques, like Gradient Descent, even when the original problem is not directly amenable to such methods.\n",
    "\n",
    "- **Example Context**: In classification tasks, the true goal is often to minimize the **0-1 loss**, which counts the number of incorrect predictions:\n",
    "  \n",
    "  $$\n",
    "  L_{0-1} = \n",
    "  \\begin{cases}\n",
    "  0 & \\text{if } \\text{prediction} = \\text{label} \\\\\n",
    "  1 & \\text{otherwise}\n",
    "  \\end{cases}\n",
    "  $$\n",
    "\n",
    "  However, 0-1 loss is not differentiable, making it hard to optimize directly.\n",
    "\n",
    "- **Common Surrogate Loss Functions**:\n",
    "  - **Logistic Loss**: Used in logistic regression and binary classification tasks.\n",
    "    \n",
    "    $$\n",
    "    L_{\\text{logistic}}(y, \\hat{y}) = \\log(1 + e^{-y \\cdot \\hat{y}})\n",
    "    $$\n",
    "  \n",
    "  - **Hinge Loss**: Common in Support Vector Machines (SVMs).\n",
    "    \n",
    "    $$\n",
    "    L_{\\text{hinge}}(y, \\hat{y}) = \\max(0, 1 - y \\cdot \\hat{y})\n",
    "    $$\n",
    "  \n",
    "  - **Cross-Entropy Loss**: Widely used in deep learning for classification tasks.\n",
    "    \n",
    "    $$\n",
    "    L_{\\text{cross-entropy}}(y, \\hat{y}) = -\\sum_{i} y_i \\log(\\hat{y}_i)\n",
    "    $$\n",
    "  \n",
    "  - **Squared Loss (MSE)**: Often used for regression tasks.\n",
    "    \n",
    "    $$\n",
    "    L_{\\text{squared}}(y, \\hat{y}) = (y - \\hat{y})^2\n",
    "    $$\n",
    "\n",
    "- **Advantages**: By using a surrogate loss:\n",
    "  - Optimization becomes tractable with gradient-based methods.\n",
    "  - It can provide better convergence properties.\n",
    "  - It ensures smooth gradients, which facilitate the training of complex models like neural networks.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **4.3 Batch and Mini-Batch**\n",
    "\n",
    "#### **Stochastic Gradient Descent (SGD)**\n",
    "- **Update**: Uses a single training sample for each gradient update.\n",
    "- **Advantages**: Faster updates, can escape local minima.\n",
    "- **Disadvantages**: Noisy, less stable convergence.\n",
    "\n",
    "#### **Mini-Batch Gradient Descent**\n",
    "- **Update**: Uses a small subset (mini-batch) of the training data for each update.\n",
    "- **Advantages**: Faster than full batch, more stable than SGD, efficient.\n",
    "- **Disadvantages**: Requires tuning mini-batch size, higher memory usage.\n",
    "\n",
    "#### **Batch Gradient Descent**\n",
    "- **Update**: Uses the entire training dataset for each gradient update.\n",
    "- **Advantages**: Stable, accurate gradient estimate.\n",
    "- **Disadvantages**: Computationally expensive, memory-intensive for large datasets.\n",
    "\n",
    "| Type                       | Update Source         | Advantages                           | Disadvantages                            |\n",
    "|----------------------------|-----------------------|--------------------------------------|------------------------------------------|\n",
    "| **SGD**                     | 1 sample             | Faster, escapes local minima         | Noisy, unstable                          |\n",
    "| **Mini-Batch**              | Small batch          | Efficient, stable, faster           | Needs tuning, more memory               |\n",
    "| **Batch**                   | Entire dataset       | Stable, accurate                    | Slow, memory-intensive                   |"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
